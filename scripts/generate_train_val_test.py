import argparse
import polars as pl
import os

def split_data(df: pl.DataFrame):
    return (
        df.filter((pl.col("timestamp") >= 60) & (pl.col("timestamp") < 3600)),
        df.filter((pl.col("timestamp") >= 3600) & (pl.col("timestamp") <= 4800)),
    )

def preprocess_df(df: pl.DataFrame) -> pl.DataFrame:
    df = df.rename({'function': 'unique_id', 'cpu': 'y'})
    df = df.with_columns([
        pl.col('timestamp').cast(pl.Int64),
        pl.col('y').cast(pl.Float64)
    ])
    return df

def generate_train_val(df: pl.DataFrame, train_set_filename, val_set_filename):
    print(f"ğŸ”§ Generating train/val datasets")
    df = preprocess_df(df)
    unique_ids = df['unique_id'].unique().to_list()
    all_timestamps = pl.DataFrame({'timestamp': range(60, 3600)})
    full_index = all_timestamps.join(pl.DataFrame({'unique_id': unique_ids}), how='cross')
    merged = full_index.join(df, on=['timestamp', 'unique_id'], how='left').fill_null(0.0)

    merged = merged.with_columns([
        (pl.col('timestamp') * 1000).cast(pl.Datetime(time_unit='ms')).alias('ds')
    ])

    train_dfs, val_dfs = [], []
    for uid in unique_ids:
        sub_df = merged.filter(pl.col('unique_id') == uid).sort('timestamp')
        timestamps = sub_df['timestamp'].unique()
        split_idx = int(0.8 * len(timestamps))
        split_time = timestamps[split_idx]
        train_dfs.append(sub_df.filter(pl.col('timestamp') < split_time))
        val_dfs.append(sub_df.filter(pl.col('timestamp') >= split_time))

    train_df = pl.concat(train_dfs)
    val_df = pl.concat(val_dfs)

    train_df.write_csv(train_set_filename)
    val_df.write_csv(val_set_filename)

    print(f"âœ… Train size: {train_df.shape}, Validation size: {val_df.shape}")
    return train_df, val_df

def generate_test(df: pl.DataFrame, test_set_filename):
    print(f"ğŸ”§ Generating test dataset")
    df = preprocess_df(df)
    unique_ids = df['unique_id'].unique().to_list()
    all_timestamps = pl.DataFrame({'timestamp': range(3600, 4801)})
    full_index = all_timestamps.join(pl.DataFrame({'unique_id': unique_ids}), how='cross')
    merged = full_index.join(df, on=['timestamp', 'unique_id'], how='left').fill_null(0.0)

    merged = merged.with_columns([
        (pl.col('timestamp') * 1000).cast(pl.Datetime(time_unit='ms')).alias('ds')
    ])

    test_dfs = [merged.filter(pl.col('unique_id') == uid).sort('timestamp') for uid in unique_ids]
    test_df = pl.concat(test_dfs)
    test_df.write_csv(test_set_filename)

    print(f"âœ… Test size: {test_df.shape}")
    return test_df

def main():
    parser = argparse.ArgumentParser(description="Process full 0-80min dataset into train/val/test.")
    parser.add_argument('--input', type=str, required=True, help="Path to full 0-80min CSV file generated by trace_analysis tool")
    parser.add_argument('--train_output', type=str, required=True, help="Output path for train set")
    parser.add_argument('--val_output', type=str, required=True, help="Output path for validation set")
    parser.add_argument('--test_output', type=str, required=True, help="Output path for test set")

    args = parser.parse_args()

    if not os.path.exists(args.input):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        return

    print(f"ğŸ“¥ Loading: {args.input}")
    df = pl.read_csv(args.input, has_header=True)
    df_0_60, df_60_80 = split_data(df)

    generate_train_val(df_0_60, args.train_output, args.val_output)
    generate_test(df_60_80, args.test_output)

if __name__ == "__main__":
    main()
